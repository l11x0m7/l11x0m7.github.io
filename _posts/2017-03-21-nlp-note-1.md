--- 
layout: post 
title: 自然语言处理——语言模型
date: 2017-03-21 
categories: blog 
tags: [NLP] 
description: 《统计自然语言处理》笔记
--- 

# 自然语言处理——语言模型

### 前言

最近又重拾了宗成庆老师的《统计自然语言处理》一书，距离上次看这本书已经过去了一年有余。而上个学期大部分时间都投入到深度学习相关的内容中了，所以一直拖到现在（当然，并不是因为闲下来了……）。

这本书算是我的自然语言处理入门之作，里面讲解的都是传统的统计计算语言学的内容。这本书作为综述类书籍，还是很不错的。一开始我花了好几个星期精读前面几章的内容，吭哧吭哧的好不容易看完一章，却忘了前面一章。于是索性把这本书配上教学视频（对某些算法相关的内容，看视频还是比看书更容易让人理解），粗略的过一遍，也算是有个大致的了解。以后需要对某个方面深挖，就知道在哪里找方法做了。

这几篇笔记是自己看书和视频时候的零碎笔记，并不是完备的知识点归纳。主要还是针对自己之前不知道的点进行记录。如果有将此类博客作为学习或快速入门的材料的同学，可能要让你们失望了。

### ngram模型

#### 问题：可能会有出现次数为0的ngram

解决方式：

1. 使用加1法：分子加1，分母加上vocab大小
2. 减值法：古德图灵法，就是把出现过ngram的概率降低，并把这部分概率之和均分给出现概率为0的ngram
3. Karz法：和古德图灵法类似，但是概率之和不是均分给各个概率为0的ngram，而是按照一定的权重计算方法
4. 绝对减值法：从每个计数r中减去同样的量（减少出现次数b次），剩余的概率量由未见事件均分
5. 线性减值方法：从每个计数r中减去与该计数成正比的量（减值函数为线性的），剩余概率量alpha被n0个未见事件均分

注：绝对减值法效果竟然比线性减值法好。

#### 问题：ngram语言模型的独立性假设前提在大多数情况下是不成立的（一个词只和前n-1个词相关）、各个领域的语言风格和内容不一样

解决方式：

1. 自适应方法：
	* 基于缓存的语言模型：在文本中刚刚出现过的一些词在后边的句子中再次出现的可能性往往较大，比标准的n-gram模型预测的概率要大
	* 基于混合方法的语言模型：由于大规模训练语料本身是异源的，来自不同领域的语料无论在主题方面，还是在风格方面，或者两者都有一定的差异，而测试语料一般是同源的。为了获得最佳性能，系统需要对不同语料有较好的性能。通常方法如下：
	
	![](http://odjt9j2ec.bkt.clouddn.com/nlp-note1-1.tiff)
	
	* 基于最大熵的语言模型
	
#### 语言模型的应用

* 分词
* 词性标注
