--- 
layout: post 
title: CS224d：DL&NLP学习笔记 Pset1题解推导
date: 2016-10-13 
categories: blog 
tags: [DeepLearning, NLP] 
description: CS224d
--- 

# CS224d：DL&NLP学习笔记 Pset1题解推导

> 做这部分练习的时候，需要知道下神经网络的基础知识，以及BP的原理，word2vec的SG和CBOW的实现、负样本损失函数的作用等。

## 一、Softfmax

(a)  
这个不难推导，直接给出

![1softmax](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-1softmax.jpg)

## 二、Neural Network Basics

![2NN](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-2NN.jpg)

![2NN2](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-2NN2.jpg)

![2NN3](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-2NN3.jpg)

> 关于(c)和(d)的BP推导我们放到后面进行完整的推导。

## 三、word2vec

对(a)、(b)、(c)、(d)进行总结概括。

![3word2vec](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-3word2vec.jpg)

![3word2vec2](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-3word2vec2.jpg)

![3word2vec3](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-3word2vec3.jpg)

![3word2vec4](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-3word2vec4.jpg)

## 四、BP的推导

> 这节是原练习没有的，可以看作是对“二”里BP的一般化扩展。
> 本节参考的思路是[UFLDL——BP推导](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)。

BP即Backprop，即误差反向传播。它的思路就是通过求导的链式法则，从而求出每个节点的变量对于误差函数的偏导数。

这节我们主要求解两个变量的导数：

* 每个神经元的权重W
* 每个神经元的偏置b

最后结合上面的导数，给出梯度下降算法和参数更新迭代策略。

推导过程如下：

![4bp](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-4bp.jpg)

![4bp1](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-4bp1.jpg)

![4bp2](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-4bp2.jpg)

![4bp3](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-4bp3.jpg)

![4bp4](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-4bp4.jpg)

![4bp5](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-4bp5.jpg)

![4bp6](http://bloglxm.oss-cn-beijing.aliyuncs.com/dpnlp-pset1-4bp6.jpg)

感谢阅读。